{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Ultralytics settings reset to default values. This may be due to a possible problem with your settings or a recent ultralytics package update. \n",
      "View settings with 'yolo settings' or at 'C:\\Users\\Viren\\AppData\\Roaming\\Ultralytics\\settings.yaml'\n",
      "Update settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "# Define the paths\n",
    "VIDEO_PATH = \"videos/D01_20240522173959.mp4\"\n",
    "MODEL_PATH = \"8s/detect/train3/weights/best.pt\"\n",
    "RESULT_VIDEO_PATH = \"results/result.mp4\"\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "# Process each frame\n",
    "def process_frame(frame: np.ndarray, _) -> np.ndarray:\n",
    "    results = model(frame, imgsz=1280)[0]\n",
    "    boxes = results.xyxy[:, :4].cpu().numpy()\n",
    "    confidences = results.xyxy[:, 4].cpu().numpy()\n",
    "    class_ids = results.xyxy[:, 5].cpu().numpy().astype(int)\n",
    "\n",
    "    detections = sv.Detections(xyxy=boxes, confidence=confidences, class_id=class_ids)\n",
    "    box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=2, text_scale=1.5)\n",
    "    labels = [f\"{model.names[class_id]} {confidence:.2f}\" for class_id, confidence in zip(class_ids, confidences)]\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Process the video\n",
    "sv.process_video(source_path=VIDEO_PATH, target_path=RESULT_VIDEO_PATH, callback=process_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(r\"c:\\Users\\Viren\\Downloads\\best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegmentationModel(\n",
      "  (model): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (2): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-1): 2 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (4): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-3): 4 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Conv(\n",
      "      (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (6): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-3): 4 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): Conv(\n",
      "      (conv): Conv2d(384, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (8): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(576, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-1): 2 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): SPPF(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (10): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (11): Concat()\n",
      "    (12): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(960, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-1): 2 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (14): Concat()\n",
      "    (15): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-1): 2 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (17): Concat()\n",
      "    (18): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-1): 2 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): Conv(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (20): Concat()\n",
      "    (21): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(576, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-1): 2 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(288, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (22): Segment(\n",
      "      (cv2): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (cv3): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (dfl): DFL(\n",
      "        (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (proto): Proto(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (upsample): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv3): Conv(\n",
      "          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cv4): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(384, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(576, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print model architecture\n",
    "print(model.model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8m-seg summary: 331 layers, 27,242,543 parameters, 0 gradients, 110.4 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(331, 27242543, 0, 110.4044032)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get model summary with detailed layers and parameters\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'segment', 'data': 'H:/no-finding-2/data.yaml', 'imgsz': 512, 'single_cls': False, 'model': 'c:\\\\Users\\\\Viren\\\\Downloads\\\\best.pt'}\n"
     ]
    }
   ],
   "source": [
    "# Access model hyperparameters\n",
    "print(model.overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Heart', 1: 'Left_Lung', 2: 'Right_Lung', 3: 'Spine', 4: 'Weasand'}\n"
     ]
    }
   ],
   "source": [
    "# Get class names the model was trained on\n",
    "print(model.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 d:\\Projects\\RODIC HMIS\\Abhimanyu Incubation Phase\\X Rays\\chest-xray-5\\train\\images\\CHNCXR_0037_0_png.rf.25cf15f5268d187efe853cbe5fce3202.jpg: 512x512 1 Heart, 1 Left_Lung, 1 Right_Lung, 1 Spine, 1 Weasand, 496.9ms\n",
      "Speed: 3.1ms preprocess, 496.9ms inference, 6.2ms postprocess per image at shape (1, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "result = model(r\"d:\\Projects\\RODIC HMIS\\Abhimanyu Incubation Phase\\X Rays\\chest-xray-5\\train\\images\\CHNCXR_0037_0_png.rf.25cf15f5268d187efe853cbe5fce3202.jpg\")\n",
    "result[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cpu\n",
      "Model size (MB): 104.36057376861572\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check model device (CPU/GPU)\n",
    "device = next(model.model.parameters()).device\n",
    "print(\"Model is on:\", device)\n",
    "\n",
    "# Get model size in MB\n",
    "torch.save(model.model.state_dict(), \"temp.pth\")\n",
    "import os\n",
    "print(\"Model size (MB):\", os.path.getsize(\"temp.pth\") / (1024 * 1024))\n",
    "os.remove(\"temp.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 2 handbags, 1 suitcase, 252.1ms\n",
      "Speed: 5.8ms preprocess, 252.1ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 1 suitcase, 145.4ms\n",
      "Speed: 0.0ms preprocess, 145.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 1 suitcase, 99.8ms\n",
      "Speed: 0.0ms preprocess, 99.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 suitcase, 98.2ms\n",
      "Speed: 2.0ms preprocess, 98.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 2 trucks, 1 handbag, 1 suitcase, 98.7ms\n",
      "Speed: 0.0ms preprocess, 98.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 1 suitcase, 99.5ms\n",
      "Speed: 0.0ms preprocess, 99.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 2 trucks, 1 handbag, 1 suitcase, 93.5ms\n",
      "Speed: 1.0ms preprocess, 93.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 2 trucks, 1 handbag, 1 suitcase, 87.0ms\n",
      "Speed: 1.5ms preprocess, 87.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 2 trucks, 1 handbag, 1 suitcase, 92.7ms\n",
      "Speed: 1.0ms preprocess, 92.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 2 trucks, 1 handbag, 2 suitcases, 84.0ms\n",
      "Speed: 2.0ms preprocess, 84.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 2 trucks, 2 handbags, 1 suitcase, 87.4ms\n",
      "Speed: 0.0ms preprocess, 87.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 1 handbag, 1 suitcase, 82.8ms\n",
      "Speed: 0.1ms preprocess, 82.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 2 handbags, 1 suitcase, 77.5ms\n",
      "Speed: 1.1ms preprocess, 77.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 3 suitcases, 244.2ms\n",
      "Speed: 29.4ms preprocess, 244.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 1 truck, 2 handbags, 2 suitcases, 173.0ms\n",
      "Speed: 0.0ms preprocess, 173.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 2 suitcases, 288.8ms\n",
      "Speed: 0.0ms preprocess, 288.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 1 suitcase, 168.5ms\n",
      "Speed: 15.6ms preprocess, 168.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 1 suitcase, 187.2ms\n",
      "Speed: 0.0ms preprocess, 187.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 261.8ms\n",
      "Speed: 0.0ms preprocess, 261.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 284.0ms\n",
      "Speed: 2.2ms preprocess, 284.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 269.3ms\n",
      "Speed: 0.0ms preprocess, 269.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 361.4ms\n",
      "Speed: 0.0ms preprocess, 361.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 214.9ms\n",
      "Speed: 0.0ms preprocess, 214.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 2 suitcases, 309.5ms\n",
      "Speed: 0.0ms preprocess, 309.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 2 suitcases, 226.3ms\n",
      "Speed: 0.7ms preprocess, 226.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 1 truck, 2 handbags, 1 suitcase, 263.7ms\n",
      "Speed: 0.7ms preprocess, 263.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 2 suitcases, 254.6ms\n",
      "Speed: 0.0ms preprocess, 254.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 2 suitcases, 219.4ms\n",
      "Speed: 2.0ms preprocess, 219.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 1 truck, 1 handbag, 2 suitcases, 250.0ms\n",
      "Speed: 0.0ms preprocess, 250.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 3 suitcases, 244.6ms\n",
      "Speed: 0.0ms preprocess, 244.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 2 suitcases, 236.9ms\n",
      "Speed: 0.0ms preprocess, 236.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 1 truck, 1 handbag, 1 suitcase, 343.3ms\n",
      "Speed: 15.0ms preprocess, 343.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 suitcase, 173.5ms\n",
      "Speed: 6.2ms preprocess, 173.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 1 suitcase, 185.1ms\n",
      "Speed: 5.0ms preprocess, 185.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 1 suitcase, 209.4ms\n",
      "Speed: 6.6ms preprocess, 209.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 249.4ms\n",
      "Speed: 0.0ms preprocess, 249.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 1 handbag, 233.1ms\n",
      "Speed: 6.2ms preprocess, 233.1ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 2 handbags, 178.2ms\n",
      "Speed: 0.0ms preprocess, 178.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 2 handbags, 178.1ms\n",
      "Speed: 6.8ms preprocess, 178.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 truck, 2 handbags, 151.6ms\n",
      "Speed: 0.0ms preprocess, 151.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 truck, 1 handbag, 250.3ms\n",
      "Speed: 2.0ms preprocess, 250.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 truck, 1 handbag, 228.9ms\n",
      "Speed: 0.0ms preprocess, 228.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3 cars, 1 truck, 1 handbag, 163.3ms\n",
      "Speed: 2.4ms preprocess, 163.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 1 truck, 1 handbag, 170.3ms\n",
      "Speed: 0.0ms preprocess, 170.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 1 truck, 1 handbag, 324.0ms\n",
      "Speed: 0.0ms preprocess, 324.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 1 truck, 1 handbag, 404.9ms\n",
      "Speed: 3.9ms preprocess, 404.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 330.2ms\n",
      "Speed: 0.0ms preprocess, 330.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 409.0ms\n",
      "Speed: 15.6ms preprocess, 409.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 1 handbag, 409.2ms\n",
      "Speed: 0.0ms preprocess, 409.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 1 truck, 2 handbags, 391.1ms\n",
      "Speed: 0.0ms preprocess, 391.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 1 truck, 2 handbags, 346.8ms\n",
      "Speed: 15.7ms preprocess, 346.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 1 truck, 2 handbags, 349.1ms\n",
      "Speed: 0.0ms preprocess, 349.1ms inference, 13.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 297.8ms\n",
      "Speed: 15.7ms preprocess, 297.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 266.4ms\n",
      "Speed: 0.0ms preprocess, 266.4ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 387.0ms\n",
      "Speed: 0.0ms preprocess, 387.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 394.3ms\n",
      "Speed: 0.0ms preprocess, 394.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 2 handbags, 435.9ms\n",
      "Speed: 0.5ms preprocess, 435.9ms inference, 8.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 422.2ms\n",
      "Speed: 10.6ms preprocess, 422.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 334.6ms\n",
      "Speed: 0.0ms preprocess, 334.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 1 truck, 2 handbags, 331.2ms\n",
      "Speed: 2.9ms preprocess, 331.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 1 chair, 284.3ms\n",
      "Speed: 1.9ms preprocess, 284.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 217.4ms\n",
      "Speed: 2.0ms preprocess, 217.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 280.6ms\n",
      "Speed: 0.0ms preprocess, 280.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 2 handbags, 1 chair, 325.5ms\n",
      "Speed: 5.3ms preprocess, 325.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 2 handbags, 1 chair, 282.0ms\n",
      "Speed: 2.0ms preprocess, 282.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 2 handbags, 1 chair, 299.5ms\n",
      "Speed: 15.7ms preprocess, 299.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 1 chair, 325.1ms\n",
      "Speed: 3.2ms preprocess, 325.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 1 truck, 1 handbag, 1 chair, 234.6ms\n",
      "Speed: 0.0ms preprocess, 234.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 truck, 1 handbag, 1 chair, 149.9ms\n",
      "Speed: 0.0ms preprocess, 149.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 2 trucks, 1 boat, 1 handbag, 1 chair, 231.6ms\n",
      "Speed: 2.2ms preprocess, 231.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 2 trucks, 1 boat, 1 handbag, 1 chair, 183.4ms\n",
      "Speed: 16.8ms preprocess, 183.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 2 trucks, 1 boat, 1 handbag, 1 chair, 148.9ms\n",
      "Speed: 0.0ms preprocess, 148.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 2 trucks, 1 boat, 1 handbag, 1 chair, 217.1ms\n",
      "Speed: 0.0ms preprocess, 217.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 2 trucks, 1 handbag, 1 chair, 282.4ms\n",
      "Speed: 2.0ms preprocess, 282.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 1 chair, 335.3ms\n",
      "Speed: 0.0ms preprocess, 335.3ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 1 chair, 355.0ms\n",
      "Speed: 0.0ms preprocess, 355.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 1 chair, 232.3ms\n",
      "Speed: 0.0ms preprocess, 232.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 267.2ms\n",
      "Speed: 0.0ms preprocess, 267.2ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 300.6ms\n",
      "Speed: 15.4ms preprocess, 300.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 335.0ms\n",
      "Speed: 0.0ms preprocess, 335.0ms inference, 15.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 184.3ms\n",
      "Speed: 2.1ms preprocess, 184.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 boat, 1 handbag, 1 chair, 400.9ms\n",
      "Speed: 1.2ms preprocess, 400.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 boat, 1 handbag, 369.6ms\n",
      "Speed: 0.0ms preprocess, 369.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 2 trucks, 1 boat, 1 handbag, 368.4ms\n",
      "Speed: 0.0ms preprocess, 368.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 car, 1 truck, 1 boat, 1 handbag, 400.6ms\n",
      "Speed: 0.0ms preprocess, 400.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 car, 1 truck, 1 handbag, 372.8ms\n",
      "Speed: 0.0ms preprocess, 372.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 car, 1 truck, 1 handbag, 352.0ms\n",
      "Speed: 0.0ms preprocess, 352.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 306.9ms\n",
      "Speed: 0.0ms preprocess, 306.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 299.5ms\n",
      "Speed: 0.0ms preprocess, 299.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 1 truck, 1 handbag, 315.5ms\n",
      "Speed: 0.0ms preprocess, 315.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 car, 1 truck, 1 handbag, 317.2ms\n",
      "Speed: 1.3ms preprocess, 317.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 car, 1 truck, 1 boat, 1 chair, 317.1ms\n",
      "Speed: 0.0ms preprocess, 317.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 319.0ms\n",
      "Speed: 0.0ms preprocess, 319.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 2 trucks, 313.8ms\n",
      "Speed: 3.9ms preprocess, 313.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 267.4ms\n",
      "Speed: 0.0ms preprocess, 267.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 2 trucks, 309.7ms\n",
      "Speed: 4.2ms preprocess, 309.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 2 trucks, 229.2ms\n",
      "Speed: 3.0ms preprocess, 229.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 1 truck, 281.6ms\n",
      "Speed: 0.0ms preprocess, 281.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 2 trucks, 1 traffic light, 346.2ms\n",
      "Speed: 5.1ms preprocess, 346.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 cars, 1 truck, 293.9ms\n",
      "Speed: 4.2ms preprocess, 293.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 1 truck, 232.2ms\n",
      "Speed: 0.0ms preprocess, 232.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 344.5ms\n",
      "Speed: 0.0ms preprocess, 344.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 1 handbag, 182.5ms\n",
      "Speed: 0.0ms preprocess, 182.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cars, 1 truck, 1 handbag, 340.7ms\n",
      "Speed: 5.9ms preprocess, 340.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 car, 1 truck, 1 handbag, 341.9ms\n",
      "Speed: 4.0ms preprocess, 341.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 2 handbags, 250.1ms\n",
      "Speed: 0.0ms preprocess, 250.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 2 handbags, 256.1ms\n",
      "Speed: 5.0ms preprocess, 256.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 1 truck, 2 handbags, 218.3ms\n",
      "Speed: 0.0ms preprocess, 218.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 car, 1 truck, 2 handbags, 289.2ms\n",
      "Speed: 4.0ms preprocess, 289.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = \"Video/v1Collapse of Overhead Crane.mp4\"  # Replace with your video file or set to 0 for webcam\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define video writer to save output (optional)\n",
    "# out = cv2.VideoWriter(\"output.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "cv2.namedWindow(\"Inference\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform inference\n",
    "    results = model(frame)\n",
    "\n",
    "    # Draw results on the frame\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box\n",
    "            conf = box.conf[0].item()  # Confidence score\n",
    "            cls = int(box.cls[0])  # Class index\n",
    "            label = f\"{model.names[cls]}: {conf:.2f}\"\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Inference\", frame)\n",
    "\n",
    "    # Write to output video\n",
    "    # out.write(frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "# out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = \"Video/v1Collapse of Overhead Crane.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cv2.namedWindow(\"Inference\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform inference (disable verbose output)\n",
    "    results = model(frame, verbose=False)\n",
    "\n",
    "    # Draw results on the frame\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            cls = int(box.cls[0])  # Class index\n",
    "            \n",
    "            # Only process detections where class ID == 1 (Person)\n",
    "            if cls == 0:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box\n",
    "                conf = box.conf[0].item()  # Confidence score\n",
    "                label = f\"{model.names[cls]}: {conf:.2f}\"\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Inference\", frame)\n",
    "\n",
    "    # Write to output video\n",
    "    # out.write(frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "# out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def process_video(model, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    cv2.namedWindow(\"Inference\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Perform inference (disable verbose output)\n",
    "        results = model(frame, verbose=False)\n",
    "\n",
    "        # Draw results on the frame\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                cls = int(box.cls[0])  # Class index\n",
    "\n",
    "                # Only process detections where class ID == 0 (Person)\n",
    "                if cls == 0:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box\n",
    "                    conf = box.conf[0].item()  # Confidence score\n",
    "                    label = f\"{model.names[cls]}: {conf:.2f}\"\n",
    "\n",
    "                    # Draw bounding box and label\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Show frame\n",
    "        cv2.imshow(\"Inference\", frame)\n",
    "\n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage:\n",
    "# process_video(your_model, \"path/to/video.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(model, \"Video/v10.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8s_person_jib_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(model, \"Video/v10.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def process_video(model, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    cv2.namedWindow(\"Inference\", cv2.WINDOW_NORMAL)\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Skip every second frame\n",
    "        frame_count += 1\n",
    "        if frame_count % 2 == 0:\n",
    "            continue\n",
    "\n",
    "        # Perform inference (disable verbose output)\n",
    "        results = model(frame, verbose=False)\n",
    "\n",
    "        # Draw results on the frame\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                cls = int(box.cls[0])  # Class index\n",
    "                conf = box.conf[0].item()  # Confidence score\n",
    "\n",
    "                # Define colors: Red for class 0, Green for class 1\n",
    "                color = (0, 0, 255) if cls == 0 else (0, 255, 0)\n",
    "\n",
    "                # Only process detections with confidence >= 0.20\n",
    "                if conf >= 0.20 and cls == 0:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box\n",
    "                    label = f\"{model.names[cls]}: {conf:.2f}\"\n",
    "\n",
    "                    # Draw bounding box and label\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Show frame\n",
    "        cv2.imshow(\"Inference\", frame)\n",
    "\n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage:\n",
    "# process_video(your_model, \"path/to/video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8s_person_jib_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(model, \"Video/v11.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(model, \"Video/v16.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "process_video(model, \"Video/v16.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def process_video(model, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    cv2.namedWindow(\"Inference\", cv2.WINDOW_NORMAL)\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Skip every second frame\n",
    "        frame_count += 1\n",
    "        if frame_count % 2 == 0:\n",
    "            continue\n",
    "\n",
    "        # Perform inference (disable verbose output)\n",
    "        # results = model(frame, verbose=False)\n",
    "        results = model.track(frame, show=False, tracker=\"bytetrack.yaml\", verbose=False)\n",
    "\n",
    "        # Draw results on the frame\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                cls = int(box.cls[0])  # Class index\n",
    "                conf = box.conf[0].item()  # Confidence score\n",
    "\n",
    "                # Define colors: Red for class 0, Green for class 1\n",
    "                color = (0, 0, 255) if cls == 0 else (0, 255, 0)\n",
    "\n",
    "                # Only process detections with confidence >= 0.20\n",
    "                if conf >= 0.20 and cls == 0:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box\n",
    "                    label = f\"{model.names[cls]}: {conf:.2f}\"\n",
    "\n",
    "                    # Draw bounding box and label\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Show frame\n",
    "        cv2.imshow(\"Inference\", frame)\n",
    "\n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage:\n",
    "# process_video(your_model, \"path/to/video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 302.4ms\n",
      "Speed: 9.9ms preprocess, 302.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 252.3ms\n",
      "Speed: 0.0ms preprocess, 252.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 332.2ms\n",
      "Speed: 0.0ms preprocess, 332.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 295.7ms\n",
      "Speed: 1.9ms preprocess, 295.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 183.6ms\n",
      "Speed: 13.5ms preprocess, 183.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 151.6ms\n",
      "Speed: 10.2ms preprocess, 151.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 220.7ms\n",
      "Speed: 0.0ms preprocess, 220.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 193.7ms\n",
      "Speed: 0.0ms preprocess, 193.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 166.5ms\n",
      "Speed: 10.7ms preprocess, 166.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 237.1ms\n",
      "Speed: 5.8ms preprocess, 237.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 184.2ms\n",
      "Speed: 2.4ms preprocess, 184.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 177.1ms\n",
      "Speed: 6.6ms preprocess, 177.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 159.0ms\n",
      "Speed: 6.0ms preprocess, 159.0ms inference, 14.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 178.0ms\n",
      "Speed: 4.0ms preprocess, 178.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 182.7ms\n",
      "Speed: 6.4ms preprocess, 182.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 198.0ms\n",
      "Speed: 0.0ms preprocess, 198.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 183.5ms\n",
      "Speed: 0.0ms preprocess, 183.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 190.3ms\n",
      "Speed: 7.7ms preprocess, 190.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 183.8ms\n",
      "Speed: 1.3ms preprocess, 183.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 180.8ms\n",
      "Speed: 9.4ms preprocess, 180.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 192.6ms\n",
      "Speed: 5.5ms preprocess, 192.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 191.4ms\n",
      "Speed: 6.0ms preprocess, 191.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 170.1ms\n",
      "Speed: 3.2ms preprocess, 170.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 180.2ms\n",
      "Speed: 4.2ms preprocess, 180.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 158.0ms\n",
      "Speed: 1.0ms preprocess, 158.0ms inference, 12.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 162.7ms\n",
      "Speed: 4.4ms preprocess, 162.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 180.7ms\n",
      "Speed: 2.9ms preprocess, 180.7ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 199.8ms\n",
      "Speed: 2.3ms preprocess, 199.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 162.8ms\n",
      "Speed: 0.0ms preprocess, 162.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 170.8ms\n",
      "Speed: 1.5ms preprocess, 170.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 178.6ms\n",
      "Speed: 0.0ms preprocess, 178.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 171.5ms\n",
      "Speed: 5.0ms preprocess, 171.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 248.7ms\n",
      "Speed: 0.0ms preprocess, 248.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 214.9ms\n",
      "Speed: 2.9ms preprocess, 214.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 149.2ms\n",
      "Speed: 0.0ms preprocess, 149.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 183.6ms\n",
      "Speed: 2.2ms preprocess, 183.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 150.4ms\n",
      "Speed: 0.0ms preprocess, 150.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 208.8ms\n",
      "Speed: 4.5ms preprocess, 208.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 161.2ms\n",
      "Speed: 2.8ms preprocess, 161.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 159.1ms\n",
      "Speed: 0.0ms preprocess, 159.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 159.7ms\n",
      "Speed: 0.0ms preprocess, 159.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 170.6ms\n",
      "Speed: 0.0ms preprocess, 170.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 160.5ms\n",
      "Speed: 7.0ms preprocess, 160.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 159.7ms\n",
      "Speed: 2.4ms preprocess, 159.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 161.3ms\n",
      "Speed: 3.9ms preprocess, 161.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 183.3ms\n",
      "Speed: 6.7ms preprocess, 183.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 181.4ms\n",
      "Speed: 0.0ms preprocess, 181.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 229.4ms\n",
      "Speed: 0.0ms preprocess, 229.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 222.3ms\n",
      "Speed: 0.0ms preprocess, 222.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 sports ball, 192.6ms\n",
      "Speed: 0.0ms preprocess, 192.6ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 224.3ms\n",
      "Speed: 9.6ms preprocess, 224.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 176.7ms\n",
      "Speed: 2.8ms preprocess, 176.7ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m process_video(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo/v16.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[36], line 26\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(model, video_path)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Perform inference (disable verbose output)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# results = model(frame, verbose=False)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrack(frame, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tracker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytetrack.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Draw results on the frame\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\Viren\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:614\u001b[0m, in \u001b[0;36mModel.track\u001b[1;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[0;32m    612\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[0;32m    613\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Viren\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:567\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32mc:\\Users\\Viren\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\Viren\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Viren\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:274\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[i]\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m: profilers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e3\u001b[39m \u001b[38;5;241m/\u001b[39m n,\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m\"\u001b[39m: profilers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e3\u001b[39m \u001b[38;5;241m/\u001b[39m n,\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m: profilers[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e3\u001b[39m \u001b[38;5;241m/\u001b[39m n,\n\u001b[0;32m    272\u001b[0m     }\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshow:\n\u001b[1;32m--> 274\u001b[0m         s[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_results(i, Path(paths[i]), im, s)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Print batch results\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32mc:\\Users\\Viren\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:352\u001b[0m, in \u001b[0;36mBasePredictor.write_results\u001b[1;34m(self, i, p, im, s)\u001b[0m\n\u001b[0;32m    350\u001b[0m     result\u001b[38;5;241m.\u001b[39msave_crop(save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrops\u001b[39m\u001b[38;5;124m\"\u001b[39m, file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtxt_path\u001b[38;5;241m.\u001b[39mstem)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshow:\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;28mstr\u001b[39m(p))\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave:\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_predicted_images(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m p\u001b[38;5;241m.\u001b[39mname), frame)\n",
      "File \u001b[1;32mc:\\Users\\Viren\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:394\u001b[0m, in \u001b[0;36mBasePredictor.show\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m    392\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mresizeWindow(p, im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# (width, height)\u001b[39;00m\n\u001b[0;32m    393\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(p, im)\n\u001b[1;32m--> 394\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "process_video(model, \"Video/v16.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def process_video(model, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    cv2.namedWindow(\"Inference\", cv2.WINDOW_NORMAL)\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Skip every second frame explicitly\n",
    "        frame_count += 2\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
    "\n",
    "        # Perform inference (disable verbose output)\n",
    "        results = model.track(frame, show=False, tracker=\"bytetrack.yaml\", verbose=False)\n",
    "\n",
    "        # Draw results on the frame\n",
    "        if results:\n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    cls = int(box.cls[0])  # Class index\n",
    "                    conf = box.conf[0].item()  # Confidence score\n",
    "\n",
    "                    # Define colors: Red for class 0, Green for class 1\n",
    "                    color = (0, 0, 255) if cls == 0 else (0, 255, 0)\n",
    "\n",
    "                    # Only process detections with confidence >= 0.20\n",
    "                    if conf >= 0.20 and cls == 0:\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box\n",
    "                        label = f\"{model.names[cls]}: {conf:.2f}\"\n",
    "\n",
    "                        # Draw bounding box and label\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Resize window to fit video frame size\n",
    "        cv2.resizeWindow(\"Inference\", frame_width, frame_height)\n",
    "        cv2.imshow(\"Inference\", frame)\n",
    "\n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "process_video(model, \"Video/v16.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process_video(model, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # # Get video properties\n",
    "    # frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    # frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    # fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # cv2.namedWindow(\"Inference\", cv2.WINDOW_NORMAL)\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Skip every second frame explicitly\n",
    "        frame_count += 2\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
    "\n",
    "        # Perform inference (disable verbose output)\n",
    "        results = model.track(frame, show=True, tracker=\"bytetrack.yaml\", verbose=False)\n",
    "\n",
    "        # Draw results on the frame\n",
    "        if results:\n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    cls = int(box.cls[0])  # Class index\n",
    "                    conf = box.conf[0].item()  # Confidence score\n",
    "\n",
    "                    # Define colors: Red for class 0, Green for class 1\n",
    "                    color = (0, 0, 255) if cls == 0 else (0, 255, 0)\n",
    "\n",
    "                    # Only process detections with confidence >= 0.20\n",
    "                    if conf >= 0.20 and cls == 0:\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box\n",
    "                        label = f\"{model.names[cls]}: {conf:.2f}\"\n",
    "\n",
    "                        # Draw bounding box and label\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Resize window to fit video frame size\n",
    "        # cv2.resizeWindow(\"Inference\", frame_width, frame_height)\n",
    "        # cv2.imshow(\"Inference\", frame)\n",
    "\n",
    "        # # Press 'q' to exit\n",
    "        # if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        #     break\n",
    "\n",
    "    # Release resources\n",
    "    # cap.release()\n",
    "    # cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "process_video(model, \"Video/v16.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
